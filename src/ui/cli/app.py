"""
Arquivo principal de execu√ß√£o da aplica√ß√£o Closest String Problem (CSP).

Este m√≥dulo orquestra o fluxo principal da aplica√ß√£o, incluindo:
    1. Sele√ß√£o e leitura/gera√ß√£o do dataset.
    2. Sele√ß√£o dos algoritmos a executar.
    3. Execu√ß√£o dos algoritmos selecionados.
    4. Exibi√ß√£o e salvamento dos resultados.

A aplica√ß√£o pode ser executada em modo interativo ou automatizado (para testes).

Attributes:
    Nenhum atributo global relevante.
"""

import argparse
import logging
import os
import signal
import sys
import traceback
import uuid
from datetime import datetime

from algorithms.base import global_registry
from src.core.io.results_formatter import ResultsFormatter
from src.core.report.report_utils import print_quick_summary
from src.ui.cli.console_manager import console
from src.ui.cli.menu import (
    configure_optimization_params,
    configure_sensitivity_params,
    menu,
    select_algorithms,
    select_optimization_algorithm,
    select_sensitivity_algorithm,
)
from src.ui.cli.save_wizard import ask_save_dataset
from src.utils.config import ALGORITHM_TIMEOUT, safe_input
from src.utils.curses_console import create_console_manager
from src.utils.logging import setup_logging
from src.utils.resource_monitor import (
    check_algorithm_feasibility,
    get_safe_memory_limit,
)

RESULTS_DIR = "results"
LOGS_DIR = "logs"
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)


def signal_handler(signum, frame):
    """Handler para sinais de interrup√ß√£o.

    Args:
        signum (int): N√∫mero do sinal recebido.
        frame (frame object): Frame atual de execu√ß√£o.
    """
    print("\n\nOpera√ß√£o cancelada pelo usu√°rio. Encerrando.")
    sys.exit(0)


def main():
    parser = argparse.ArgumentParser(description="Closest String Problem (CSP) - Execu√ß√£o principal")
    parser.add_argument(
        "--silent",
        action="store_true",
        help="Executa em modo silencioso (sem prints interativos)",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=["synthetic", "file", "entrez", "batch"],
        help="Fonte do dataset",
    )
    parser.add_argument(
        "--algorithms",
        type=str,
        nargs="+",
        help="Algoritmos a executar (nomes separados por espa√ßo)",
    )
    parser.add_argument("--num-execs", type=int, help="N√∫mero de execu√ß√µes por algoritmo")
    parser.add_argument("--timeout", type=int, help="Timeout por execu√ß√£o (segundos)")
    parser.add_argument("--workers", "-w", type=int, default=4, help="N√∫mero de workers paralelos (padr√£o: 4)")
    parser.add_argument(
        "--console",
        type=str,
        choices=["auto", "curses", "simple"],
        default="auto",
        help="Tipo de console: auto (detecta automaticamente), curses (for√ßar curses), simple (console simples)",
    )
    args = parser.parse_args()

    silent = args.silent

    # Criar console manager apropriado
    if silent:
        # Em modo silencioso, sempre usar console simples
        console_manager = create_console_manager(force_simple=True)
    else:
        # Detectar ou for√ßar tipo de console
        if args.console == "simple":
            console_manager = create_console_manager(force_simple=True)
        elif args.console == "curses":
            console_manager = create_console_manager(force_simple=False)
        else:  # auto
            console_manager = create_console_manager(force_simple=False)

    def silent_print(*a, **k):
        pass

    p = print if not silent else silent_print
    cprint = console_manager.print if not silent else silent_print

    # Mostrar o n√∫mero do processo (PID) logo no in√≠cio
    p(f"[PID] Processo em execu√ß√£o: {os.getpid()}")

    # Configurar handlers de sinal para sa√≠da limpa
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        uid = uuid.uuid4().hex[:8]
        base_name = f"{ts}_{uid}"
        # Passa silent para setup_logging
        setup_logging(base_name, silent=silent)

        # Se modo silencioso, use defaults para qualquer par√¢metro n√£o informado
        if silent:
            if not args.dataset:
                args.dataset = "synthetic"
            if not args.algorithms:
                args.algorithms = ["Baseline"]  # Usar Baseline em vez de BLF-GA para testes
            if not args.num_execs:
                args.num_execs = 1
            if not args.timeout:
                args.timeout = ALGORITHM_TIMEOUT

        # Dataset
        params = {}
        seqs = []
        seed = None

        # Escolha do dataset
        if args.dataset:
            choice = args.dataset
            if args.dataset == "synthetic":
                from src.datasets.dataset_synthetic import generate_dataset

                seqs, p = generate_dataset(silent=silent)
                logging.debug(f"[main] Par√¢metros do dataset: {p}")
                params = {"dataset_source": "1"}
                params.update(p)
            elif args.dataset == "file":
                from src.datasets.dataset_file import load_dataset

                seqs, p = load_dataset(silent=silent)
                params = {"dataset_source": "2"}
                params.update(p)
            elif args.dataset == "entrez":
                from src.datasets.dataset_entrez import fetch_dataset

                seqs, p = fetch_dataset()
                params = {"dataset_source": "3"}
                params.update(p)
            elif args.dataset == "batch":
                from src.core.exec.batch_executor import (
                    BatchExecutor,
                    select_batch_config,
                )

                # Em modo silencioso, usar o arquivo de teste
                if silent:
                    config_file = "batch_configs/teste.yaml"
                else:
                    config_file = select_batch_config()
                    if not config_file:
                        cprint("‚ùå Nenhum arquivo de configura√ß√£o selecionado.")
                        return
                executor = BatchExecutor(config_file, workers=args.workers)
                batch_result = executor.execute_batch()
                cprint("\n‚úÖ Execu√ß√£o em lote conclu√≠da!")
                cprint(f"Tempo total: {batch_result['tempo_total']:.1f}s")
                cprint(f"Taxa de sucesso: {batch_result['resumo']['taxa_sucesso']:.1f}%")
                from src.core.io.exporter import CSPExporter

                batch_dir = executor.results_dir
                import os as os_batch

                json_path = os_batch.path.join(batch_dir, "batch_results.json")
                csv_path = os_batch.path.join(batch_dir, "batch_results.csv")
                if os_batch.path.exists(json_path):
                    exporter = CSPExporter()
                    exporter.export_batch_json_to_csv(json_path, csv_path)
                    cprint(f"üìÑ Resultados detalhados do batch exportados para CSV: {csv_path}")
                return
            else:
                cprint("‚ùå Fonte de dataset inv√°lida.")
                return
        else:
            # Fluxo interativo
            choice = menu()
            params = {"dataset_source": choice}

            if choice == "4":
                from src.core.exec.batch_executor import (
                    BatchExecutor,
                    select_batch_config,
                )

                config_file = select_batch_config()
                if not config_file:
                    cprint("‚ùå Nenhum arquivo de configura√ß√£o selecionado.")
                    return
                try:
                    executor = BatchExecutor(config_file, workers=args.workers)
                    batch_result = executor.execute_batch()
                    cprint("\n‚úÖ Execu√ß√£o em lote conclu√≠da!")
                    cprint(f"Tempo total: {batch_result['tempo_total']:.1f}s")
                    cprint(f"Taxa de sucesso: {batch_result['resumo']['taxa_sucesso']:.1f}%")
                    from src.core.io.exporter import CSPExporter

                    batch_dir = executor.results_dir
                    import os as os_batch

                    json_path = os_batch.path.join(batch_dir, "batch_results.json")
                    csv_path = os_batch.path.join(batch_dir, "batch_results.csv")
                    if os_batch.path.exists(json_path):
                        exporter = CSPExporter()
                        exporter.export_batch_json_to_csv(json_path, csv_path)
                        cprint(f"üìÑ Resultados detalhados do batch exportados para CSV: {csv_path}")
                except Exception as e:
                    cprint(f"‚ùå Erro na execu√ß√£o em lote: {e}")
                    return
            elif choice == "5":
                # Execu√ß√£o em lote com interface curses
                from src.ui.curses_integration import add_curses_batch_option_to_menu

                curses_executor = add_curses_batch_option_to_menu()
                curses_executor()
                return
            elif choice == "6":
                # Otimiza√ß√£o de hiperpar√¢metros
                try:
                    run_optimization_workflow()
                except Exception as e:
                    cprint(f"‚ùå Erro na otimiza√ß√£o: {e}")
                    logging.exception("Erro na otimiza√ß√£o", exc_info=e)
                return
            elif choice == "7":
                # An√°lise de sensibilidade
                try:
                    run_sensitivity_workflow()
                except Exception as e:
                    cprint(f"‚ùå Erro na an√°lise de sensibilidade: {e}")
                    logging.exception("Erro na an√°lise de sensibilidade", exc_info=e)
                return
            elif choice in ["1", "2", "3"]:
                try:
                    if choice == "1":
                        from src.datasets.dataset_synthetic import generate_dataset

                        seqs, p = generate_dataset(silent=silent)
                        logging.debug(f"[main] Par√¢metros do dataset: {p}")
                        params.update(p)
                        if not silent:
                            ask_save_dataset(seqs, "synthetic", p)
                    elif choice == "2":
                        from src.datasets.dataset_file import load_dataset

                        seqs, p = load_dataset(silent=silent)
                        params.update(p)
                    elif choice == "3":
                        from src.datasets.dataset_entrez import fetch_dataset

                        seqs, p = fetch_dataset()
                        params.update(p)
                        ask_save_dataset(seqs, "entrez", p)
                except Exception as exc:
                    cprint(f"Erro: {exc}")
                    logging.exception("Erro ao carregar dataset", exc_info=exc)
                    return

        # Ap√≥s carregar o dataset, extrair informa√ß√µes extras se dispon√≠veis
        seed = params.get("seed")
        logging.debug(f"[main] seed: {seed}")

        # Exibir dist√¢ncia da string base se dispon√≠vel
        distancia_base = params.get("distancia_string_base")
        if distancia_base is not None:
            cprint(f"Dist√¢ncia da string base: {distancia_base}")

        if not seqs:
            cprint("Nenhuma sequ√™ncia lida.")
            return

        alphabet = "".join(sorted(set("".join(seqs))))
        n, L = len(seqs), len(seqs[0])
        cprint(f"\nDataset: n={n}, L={L}, |Œ£|={len(alphabet)}")

        # Log simplificado do dataset
        logging.debug(f"[DATASET] n={n}, L={L}, |Œ£|={len(alphabet)}")
        if len(seqs) <= 5:
            logging.debug(f"[DATASET] Strings: {seqs}")
        else:
            logging.debug(f"[DATASET] {len(seqs)} strings (primeiras 2: {seqs[:2]})")

        # Verifica√ß√£o de recursos do sistema
        safe_memory = get_safe_memory_limit()
        cprint(f"Limite seguro de mem√≥ria: {safe_memory:.1f}%")

        # Algoritmos
        if args.algorithms:
            algs = args.algorithms
        else:
            algs = select_algorithms()
        if not algs:
            cprint("Nenhum algoritmo selecionado.")
            return

        # Verificar viabilidade dos algoritmos selecionados
        viable_algs = []
        for alg_name in algs:
            is_viable, msg = check_algorithm_feasibility(n, L, alg_name)
            if is_viable:
                viable_algs.append(alg_name)
                cprint(f"‚úì {alg_name}: {msg}")
            else:
                cprint(f"‚ö† {alg_name}: {msg} (ser√° pulado)")

        if not viable_algs:
            cprint("Nenhum algoritmo vi√°vel.")
            return

        # N√∫mero de execu√ß√µes e timeout
        if args.num_execs:
            num_execs = args.num_execs
        else:
            runs = safe_input("\nN¬∫ execu√ß√µes p/ algoritmo [3]: ")
            num_execs = int(runs) if runs.isdigit() and int(runs) > 0 else 3
        if args.timeout:
            timeout = args.timeout
        else:
            default_timeout = ALGORITHM_TIMEOUT
            timeout_input = safe_input(f"\nTimeout por execu√ß√£o em segundos [{default_timeout}]: ")
            timeout = int(timeout_input) if timeout_input.isdigit() and int(timeout_input) > 0 else default_timeout
        cprint(f"Timeout configurado: {timeout}s por execu√ß√£o")

        # Execu√ß√£o dos algoritmos
        cprint("\n" + "=" * 50)
        cprint("EXECUTANDO ALGORITMOS")
        cprint("=" * 50)

        # Configurar workers internos baseado no n√∫mero de CPUs e workers externos
        import multiprocessing

        cpu_count = multiprocessing.cpu_count()
        external_workers = args.workers

        # Verificar se algum algoritmo suporta paralelismo interno
        has_internal_parallel = any(
            getattr(global_registry.get(alg_name), "supports_internal_parallel", False)
            for alg_name in viable_algs
            if alg_name in global_registry
        )

        if has_internal_parallel:
            # Se h√° paralelismo interno, use apenas 1 worker externo
            external_workers = 1
            internal_workers = max(1, cpu_count // 1)
        else:
            # Se n√£o h√° paralelismo interno, use todos os workers externos
            internal_workers = max(1, cpu_count // external_workers)

        # Configurar vari√°vel de ambiente para workers internos
        os.environ["INTERNAL_WORKERS"] = str(internal_workers)

        cprint("üîß Configura√ß√£o de paralelismo:")
        cprint(f"   - CPUs dispon√≠veis: {cpu_count}")
        cprint(f"   - Workers externos: {external_workers}")
        cprint(f"   - Workers internos: {internal_workers}")
        cprint(f"   - Paralelismo interno detectado: {has_internal_parallel}")

        # Decidir se usar execu√ß√£o paralela ou sequencial
        use_parallel = len(viable_algs) > 1 and num_execs == 1

        if use_parallel:
            cprint(f"üöÄ Usando execu√ß√£o paralela para {len(viable_algs)} algoritmos")
            from src.core.exec.parallel_runner import execute_algorithms_parallel

            parallel_results = execute_algorithms_parallel(
                algorithm_names=viable_algs,
                seqs=seqs,
                alphabet=alphabet,
                console=console_manager,
                max_workers=external_workers,
                timeout=timeout,
            )

            formatter = ResultsFormatter()
            results = {}

            for alg_name, alg_data in parallel_results.items():
                # Converter resultado do formato paralelo para o formato esperado
                executions = [alg_data]  # Resultado paralelo j√° √© uma execu√ß√£o
                executions[0]["seed"] = seed

                formatter.add_algorithm_results(alg_name, executions)

                if "distancia" in alg_data and alg_data["distancia"] != float("inf"):
                    dist_base = params.get("distancia_string_base", "-")
                    results[alg_name] = {
                        "dist": alg_data["distancia"],
                        "dist_base": dist_base,
                        "time": alg_data["tempo"],
                    }
                else:
                    results[alg_name] = {
                        "dist": "-",
                        "dist_base": "-",
                        "time": "-",
                    }
        else:
            cprint(f"üîÑ Usando execu√ß√£o sequencial para {len(viable_algs)} algoritmos")
            from src.core.exec.runner import execute_algorithm_runs

            formatter = ResultsFormatter()
            results = {}

            for alg_name in viable_algs:
                if alg_name not in global_registry:
                    cprint(f"ERRO: Algoritmo '{alg_name}' n√£o encontrado!")
                    continue

                # Log apenas in√≠cio simplificado
                logging.debug(f"[ALG_EXEC] Iniciando {alg_name}")
                AlgClass = global_registry[alg_name]

                executions = execute_algorithm_runs(
                    alg_name, AlgClass, seqs, alphabet, num_execs, None, console_manager, timeout
                )

                # Log resumido das execu√ß√µes
                logging.debug(f"[ALG_EXEC] {alg_name} conclu√≠do: {len(executions)} execu√ß√µes")
                for _, exec_data in enumerate(executions):
                    # N√£o calcular mais distancia_string_base aqui, apenas usar seed
                    exec_data["seed"] = seed

                formatter.add_algorithm_results(alg_name, executions)
                valid_results = [e for e in executions if "distancia" in e and e["distancia"] != float("inf")]
                if valid_results:
                    best_exec = min(valid_results, key=lambda e: e["distancia"])
                    logging.debug(f"[ALG_EXEC] {alg_name} melhor: dist={best_exec['distancia']}")

                    # Adicionar dist√¢ncia da string base ao resultado
                    dist_base = params.get("distancia_string_base", "-")

                    results[alg_name] = {
                        "dist": best_exec["distancia"],
                        "dist_base": dist_base,
                        "time": best_exec["tempo"],
                    }
                else:
                    error_exec = next((e for e in executions if "erro" in e), executions[0])
                    logging.debug(f"[ALG_EXEC] {alg_name} sem resultados v√°lidos")
                    results[alg_name] = {
                        "dist": "-",
                        "dist_base": "-",
                        "time": error_exec["tempo"],
                        "warn": error_exec.get("erro", "Erro desconhecido"),
                    }

        # Exibir resumo dos resultados
        print_quick_summary(results, console_manager)

        cprint("\nüìÑ Gerando relat√≥rio detalhado...")
        # Adicionar informa√ß√µes b√°sicas ao formatter para o relat√≥rio
        if hasattr(formatter, "__dict__"):
            # Captura todas as strings base e suas dist√¢ncias
            base_strings_info = []
            for _, execs in formatter.results.items():
                for exec_data in execs:
                    if exec_data.get("melhor_string"):
                        base_strings_info.append(
                            {
                                "base_string": exec_data["melhor_string"],
                                "distancia_string_base": params.get("distancia_string_base", "-"),
                            }
                        )
            formatter.extra_info = {
                "seed": seed,
                "params": params,
                "dataset_strings": seqs,
                "base_strings_info": base_strings_info,
            }
            logging.debug("[main] formatter configurado")

        results_dir = "results"
        txt_path = os.path.join(results_dir, f"{base_name}.txt")
        csv_path = os.path.join(results_dir, f"{base_name}.csv")

        formatter.save_detailed_report(txt_path)

        # Salvar resultados detalhados em CSV
        formatter.export_to_csv(csv_path)
        cprint(f"üìÑ Resultados detalhados exportados para CSV: {csv_path}")

        # Sucesso - retornar 0 explicitamente
        if silent:
            sys.exit(0)

    except Exception as e:
        if not silent:
            console_manager.print(f"\nERRO FATAL: {e}")
            traceback.print_exc()
        else:
            logging.exception("Erro fatal durante execu√ß√£o", exc_info=e)
        sys.exit(1)
    finally:
        # Limpar console curses se necess√°rio
        if hasattr(console_manager, "cleanup") and callable(getattr(console_manager, "cleanup", None)):
            console_manager.cleanup()


def generate_dataset_automated():
    """Gera dataset sint√©tico com valores padr√£o para testes automatizados.

    Utiliza par√¢metros fixos para garantir reprodutibilidade em testes.

    Returns:
        tuple: Lista de strings do dataset e dicion√°rio de par√¢metros utilizados.
    """
    from src.datasets.dataset_synthetic import SYNTHETIC_DEFAULTS

    n = SYNTHETIC_DEFAULTS["n"]
    L = SYNTHETIC_DEFAULTS["L"]
    alphabet = SYNTHETIC_DEFAULTS["alphabet"]
    noise = SYNTHETIC_DEFAULTS["noise"]
    fully_random = False
    seed = 42
    params = {
        "n": n,
        "L": L,
        "alphabet": alphabet,
        "noise": noise,
        "fully_random": fully_random,
        "seed": seed,
    }
    import random

    rng = random.Random(seed)
    base_string = "".join(rng.choices(alphabet, k=L))
    data = []
    for _ in range(n):
        s = list(base_string)
        num_mut = int(round(noise * L))
        mut_pos = rng.sample(range(L), num_mut) if num_mut > 0 else []
        for pos in mut_pos:
            orig = s[pos]
            alt = rng.choice([c for c in alphabet if c != orig])
            s[pos] = alt
        new_s = "".join(s)
        data.append(new_s)
    return data, params


def run_optimization_workflow():
    """Executa o workflow de otimiza√ß√£o de hiperpar√¢metros."""
    from src.datasets.dataset_synthetic import generate_dataset
    from src.optimization.optuna_optimizer import optimize_algorithm

    console.print("\n=== Otimiza√ß√£o de Hiperpar√¢metros ===")

    # Selecionar algoritmo
    algorithm_name = select_optimization_algorithm()
    if not algorithm_name:
        console.print("‚ùå Nenhum algoritmo selecionado.")
        return

    # Configurar par√¢metros
    config = configure_optimization_params()

    # Gerar dataset para otimiza√ß√£o
    console.print("\nüìä Gerando dataset para otimiza√ß√£o...")
    seqs, _ = generate_dataset(silent=True)

    console.print(f"‚úÖ Dataset gerado: {len(seqs)} sequ√™ncias de tamanho {len(seqs[0])}")

    # Executar otimiza√ß√£o
    alphabet = "".join(sorted(set("".join(seqs))))

    try:
        result = optimize_algorithm(
            algorithm_name=algorithm_name,
            sequences=seqs,
            alphabet=alphabet,
            n_trials=config["n_trials"],
            timeout_per_trial=config["timeout"],
            show_progress=True,
        )

        console.print("\n‚úÖ Otimiza√ß√£o conclu√≠da!")
        console.print(f"Melhor valor: {result.best_value}")
        console.print(f"Melhores par√¢metros: {result.best_params}")

        # Salvar visualiza√ß√µes se solicitado
        if config["save_plots"]:
            from src.optimization.visualization import OptimizationVisualizer

            visualizer = OptimizationVisualizer(result)
            plots_dir = os.path.join(RESULTS_DIR, "optimization_plots")
            os.makedirs(plots_dir, exist_ok=True)

            # Salvar gr√°ficos
            history_path = os.path.join(plots_dir, f"{algorithm_name}_history.png")
            importance_path = os.path.join(plots_dir, f"{algorithm_name}_importance.png")

            visualizer.plot_optimization_history(save_path=history_path)
            visualizer.plot_parameter_importance(save_path=importance_path)

            console.print(f"üìä Gr√°ficos salvos em: {plots_dir}")

        # Salvar resultados
        import json

        results_path = os.path.join(
            RESULTS_DIR, f"optimization_{algorithm_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(results_path, "w") as f:
            json.dump(
                {
                    "best_params": result.best_params,
                    "best_value": result.best_value,
                    "n_trials": result.n_trials,
                    "study_name": result.study_name,
                    "optimization_time": result.optimization_time,
                    "all_trials": result.all_trials,
                },
                f,
                indent=2,
                default=str,
            )

        console.print(f"üíæ Resultados salvos em: {results_path}")

    except Exception as e:
        console.print(f"‚ùå Erro durante otimiza√ß√£o: {e}")
        logging.exception("Erro na otimiza√ß√£o", exc_info=e)


def run_sensitivity_workflow():
    """Executa o workflow de an√°lise de sensibilidade."""
    from src.datasets.dataset_synthetic import generate_dataset
    from src.optimization.sensitivity_analyzer import analyze_algorithm_sensitivity

    console.print("\n=== An√°lise de Sensibilidade ===")

    # Selecionar algoritmo
    algorithm_name = select_sensitivity_algorithm()
    if not algorithm_name:
        console.print("‚ùå Nenhum algoritmo selecionado.")
        return

    # Configurar par√¢metros
    config = configure_sensitivity_params()

    # Gerar dataset para an√°lise
    console.print("\nüìä Gerando dataset para an√°lise...")
    seqs, _ = generate_dataset(silent=True)

    console.print(f"‚úÖ Dataset gerado: {len(seqs)} sequ√™ncias de tamanho {len(seqs[0])}")

    # Executar an√°lise de sensibilidade
    alphabet = "".join(sorted(set("".join(seqs))))

    try:
        result = analyze_algorithm_sensitivity(
            algorithm_name=algorithm_name,
            sequences=seqs,
            alphabet=alphabet,
            n_samples=config["n_samples"],
            timeout_per_sample=config.get("timeout", 60),
            show_progress=True,
        )

        console.print("\n‚úÖ An√°lise de sensibilidade conclu√≠da!")
        console.print(f"Par√¢metros analisados: {len(result.parameter_names)}")

        # Mostrar principais par√¢metros sens√≠veis
        important_params = result.get_most_important_parameters(n=5)
        console.print("\nüìà Par√¢metros mais sens√≠veis:")
        for param, value in important_params:
            console.print(f"  ‚Ä¢ {param}: {value:.4f}")

        # Salvar visualiza√ß√µes se solicitado
        if config["save_plots"]:
            from src.optimization.visualization import SensitivityVisualizer

            visualizer = SensitivityVisualizer(result)
            plots_dir = os.path.join(RESULTS_DIR, "sensitivity_plots")
            os.makedirs(plots_dir, exist_ok=True)

            # Salvar gr√°ficos
            sensitivity_path = os.path.join(plots_dir, f"{algorithm_name}_sensitivity.png")
            visualizer.plot_sensitivity_indices(save_path=sensitivity_path)

            console.print(f"üìä Gr√°ficos salvos em: {plots_dir}")

        # Salvar resultados
        import json

        results_path = os.path.join(
            RESULTS_DIR, f"sensitivity_{algorithm_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(results_path, "w") as f:
            json.dump(
                {
                    "method": result.method,
                    "parameter_names": result.parameter_names,
                    "first_order": result.first_order,
                    "total_order": result.total_order,
                    "second_order": result.second_order,
                    "confidence_intervals": result.confidence_intervals,
                    "n_samples": result.n_samples,
                    "analysis_time": result.analysis_time,
                },
                f,
                indent=2,
                default=str,
            )

        console.print(f"üíæ Resultados salvos em: {results_path}")

    except Exception as e:
        console.print(f"‚ùå Erro durante an√°lise de sensibilidade: {e}")
        logging.exception("Erro na an√°lise de sensibilidade", exc_info=e)
